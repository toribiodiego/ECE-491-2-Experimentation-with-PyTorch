{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate --quiet"
      ],
      "metadata": {
        "collapsed": true,
        "id": "n17CtydqmrFs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import timm\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "\n",
        "def save_checkpoint(model, output_path):\n",
        "    \"\"\"Save the model state_dict to the specified output path.\"\"\"\n",
        "    torch.save(model.state_dict(), os.path.join(output_path, \"pytorch_model.bin\"))\n",
        "    print(f\"Checkpoint saved at {output_path}\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute accuracy using the evaluate package.\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "class CustomTrainer:\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, eval_loader, compute_metrics, device, fp16=False):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.eval_loader = eval_loader\n",
        "        self.compute_metrics = compute_metrics\n",
        "        self.device = device\n",
        "        self.fp16 = fp16\n",
        "        if self.fp16:\n",
        "            self.scaler = torch.amp.GradScaler()\n",
        "        else:\n",
        "            self.scaler = None\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        self.model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for step, batch in enumerate(self.train_loader):\n",
        "            inputs = batch[\"pixel_values\"].to(self.device)\n",
        "            labels = batch[\"labels\"].to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            if self.fp16:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = self.model(inputs)\n",
        "                    loss = self.criterion(logits, labels)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                logits = self.model(inputs)\n",
        "                loss = self.criterion(logits, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f\"Step {step+1}/{len(self.train_loader)} - Loss: {loss.item():.4f}\")\n",
        "        avg_loss = epoch_loss / len(self.train_loader)\n",
        "        print(f\"Epoch {epoch} training loss: {avg_loss:.4f}\")\n",
        "        return avg_loss\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        all_logits = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in self.eval_loader:\n",
        "                inputs = batch[\"pixel_values\"].to(self.device)\n",
        "                labels = batch[\"labels\"].to(self.device)\n",
        "                logits = self.model(inputs)\n",
        "                all_logits.append(logits.cpu())\n",
        "                all_labels.append(labels.cpu())\n",
        "        all_logits = torch.cat(all_logits, dim=0).numpy()\n",
        "        all_labels = torch.cat(all_labels, dim=0).numpy()\n",
        "        metrics = self.compute_metrics((all_logits, all_labels))\n",
        "        print(\"Evaluation Metrics:\", metrics)\n",
        "        return metrics\n",
        "\n",
        "def get_dataloaders(train_dataset, eval_dataset, train_batch_size, eval_batch_size):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "    return train_loader, eval_loader\n",
        "\n",
        "def load_components(config):\n",
        "    dataset = load_dataset(config[\"dataset_name\"])\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                             std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    def transform_fn(example):\n",
        "        img = example[\"img\"]\n",
        "        if isinstance(img, list):\n",
        "            img = img[0]\n",
        "        if not isinstance(img, Image.Image):\n",
        "            img = Image.fromarray(img)\n",
        "        img = img.convert(\"RGB\")\n",
        "        tensor = transform(img)\n",
        "        if tensor.ndim == 2:\n",
        "            tensor = tensor.unsqueeze(0).repeat(3, 1, 1)\n",
        "        example[\"pixel_values\"] = tensor\n",
        "        example[\"labels\"] = example[\"fine_label\"]\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(transform_fn)\n",
        "\n",
        "    split_datasets = dataset[\"train\"].train_test_split(test_size=config[\"validation_split\"], seed=config[\"seed\"])\n",
        "    train_dataset = split_datasets[\"train\"]\n",
        "    eval_dataset = split_datasets[\"test\"]\n",
        "\n",
        "    train_dataset.set_format(\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
        "    eval_dataset.set_format(\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
        "\n",
        "    sample = train_dataset[0]\n",
        "    print(\"\\n--- Sample from train dataset after transformation ---\")\n",
        "    print(\"Sample 'pixel_values' type:\", type(sample[\"pixel_values\"]))\n",
        "    print(\"Sample 'pixel_values' shape:\", sample[\"pixel_values\"].shape)\n",
        "    print(\"Sample label:\", sample[\"labels\"])\n",
        "\n",
        "    model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=config[\"num_labels\"])\n",
        "    return train_dataset, eval_dataset, model\n",
        "\n",
        "\n",
        "def main():\n",
        "    config = {\n",
        "        \"dataset_name\": \"cifar100\",\n",
        "        \"num_labels\": 100,\n",
        "        \"validation_split\": 0.1,\n",
        "        \"seed\": 42,\n",
        "        \"output_dir\": \"./vit-cifar100-checkpoints\",\n",
        "        \"per_device_train_batch_size\": 16,\n",
        "        \"per_device_eval_batch_size\": 32,\n",
        "        \"num_train_epochs\": 1,\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"weight_decay\": 0.05,\n",
        "        \"fp16\": True,\n",
        "    }\n",
        "    os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
        "    train_dataset, eval_dataset, model = load_components(config)\n",
        "    train_loader, eval_loader = get_dataloaders(train_dataset, eval_dataset,\n",
        "                                                config[\"per_device_train_batch_size\"],\n",
        "                                                config[\"per_device_eval_batch_size\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "    trainer = CustomTrainer(model, criterion, optimizer, train_loader, eval_loader, compute_metrics, device, fp16=config[\"fp16\"])\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    num_epochs = config[\"num_train_epochs\"]\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "        trainer.train_epoch(epoch)\n",
        "        metrics = trainer.evaluate()\n",
        "        current_metric = metrics.get(\"accuracy\")\n",
        "        if current_metric is not None and current_metric > best_accuracy:\n",
        "            best_accuracy = current_metric\n",
        "            checkpoint_path = os.path.join(config[\"output_dir\"], f\"best_model_{current_metric:.4f}\")\n",
        "            os.makedirs(checkpoint_path, exist_ok=True)\n",
        "            save_checkpoint(model, checkpoint_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "08e36e14456e4edbb79e6a94bd4d93fd",
            "fd44956604304637ba21a61113ec7c67",
            "28e5fac417b1423bbf34c8dfcfd42970",
            "c53f4277a7d54643b8b756802f1f39d1",
            "37b7e5f99b8d491f93605d24d7b19f85",
            "650ebd0c1ba242af8525294296429f3f",
            "5626b59c0ba141a992894a45ba78b753",
            "8acff30d4028451bb9594ab8efdd99fb",
            "07e8abfd20774f4c8ef638ae3adfa158",
            "b66eacb639064944a4f1914e6abfd751",
            "22c781c475364129b6bc62bfdbc417e7",
            "a928810326e4466a8f716941f3fd3401",
            "b794b589250f4913a2bcd3a07d3ea84b",
            "81ba1b36a9f84331b75117e23f2b228b",
            "3129f60e88e64ff5bad2e2637d4cf718",
            "c6e82d713f354780ad13af987991f988",
            "11629239e8484343892b9ee09829d185",
            "c8e220ac7fe94b1d91d4a31afb744438",
            "62c4e7c9828b4da183bf3f9dbe867290",
            "769b648ceb5749c8b7d470a01ec7a357",
            "f1500e34f71f4fa49a1041c063742c3c",
            "56cf47a625af4e4a8f569f816337391b"
          ]
        },
        "id": "4kJRlRvXm2Jg",
        "outputId": "e9b6acc8-1ec6-4f6a-eca1-755cf705305b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08e36e14456e4edbb79e6a94bd4d93fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a928810326e4466a8f716941f3fd3401"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample from train dataset after transformation ---\n",
            "Sample 'pixel_values' type: <class 'torch.Tensor'>\n",
            "Sample 'pixel_values' shape: torch.Size([3, 224, 224])\n",
            "Sample label: tensor(47)\n",
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d2019d0d10ef>:54: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/2813 - Loss: 2.3994\n",
            "Step 200/2813 - Loss: 0.3684\n",
            "Step 300/2813 - Loss: 0.6844\n",
            "Step 400/2813 - Loss: 0.4878\n",
            "Step 500/2813 - Loss: 0.7856\n",
            "Step 600/2813 - Loss: 2.6139\n",
            "Step 700/2813 - Loss: 0.4106\n",
            "Step 800/2813 - Loss: 0.6817\n",
            "Step 900/2813 - Loss: 0.1805\n",
            "Step 1000/2813 - Loss: 1.0217\n",
            "Step 1100/2813 - Loss: 0.6897\n",
            "Step 1200/2813 - Loss: 0.8167\n",
            "Step 1300/2813 - Loss: 0.4020\n",
            "Step 1400/2813 - Loss: 0.6790\n",
            "Step 1500/2813 - Loss: 0.9043\n",
            "Step 1600/2813 - Loss: 0.5107\n",
            "Step 1700/2813 - Loss: 0.5574\n",
            "Step 1800/2813 - Loss: 0.5841\n",
            "Step 1900/2813 - Loss: 1.2514\n",
            "Step 2000/2813 - Loss: 0.9650\n",
            "Step 2100/2813 - Loss: 0.5962\n",
            "Step 2200/2813 - Loss: 0.2033\n",
            "Step 2300/2813 - Loss: 0.7170\n",
            "Step 2400/2813 - Loss: 0.4085\n",
            "Step 2500/2813 - Loss: 1.6168\n",
            "Step 2600/2813 - Loss: 0.7278\n",
            "Step 2700/2813 - Loss: 0.7440\n",
            "Step 2800/2813 - Loss: 0.5184\n",
            "Epoch 1 training loss: 0.8288\n",
            "Evaluation Metrics: {'accuracy': 0.8458}\n",
            "Checkpoint saved at ./vit-cifar100-checkpoints/best_model_0.8458\n"
          ]
        }
      ]
    }
  ]
}